{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADS 509 Module 3: Group Comparison \n",
    "\n",
    "The task of comparing two groups of text is fundamental to textual analysis. There are innumerable applications: survey respondents from different segments of customers, speeches by different political parties, words used in Tweets by different constituencies, etc. In this assignment you will build code to effect comparisons between groups of text data, using the ideas learned in reading and lecture.\n",
    "\n",
    "This assignment asks you to analyze the lyrics and Twitter descriptions for the two artists.\n",
    "\n",
    "* Read in the data, normalize the text, and tokenize it. When you tokenize your Twitter descriptions, keep hashtags and emojis in your token set. \n",
    "* Calculate descriptive statistics on the two sets of lyrics and compare the results. \n",
    "* For each of the four corpora, find the words that are unique to that corpus. \n",
    "* Build word clouds for all four corpora. \n",
    "\n",
    "Each one of the analyses has a section dedicated to it below. Before beginning the analysis there is a section for you to read in the data and do your cleaning (tokenization and normalization). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install wordcloud\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this space for any additional import statements you need\n",
    "\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place any addtional functions or constants you need here. \n",
    "\n",
    "# Some punctuation variations\n",
    "punctuation = set(punctuation) # speeds up comparison\n",
    "tw_punct = punctuation - {\"#\"}\n",
    "\n",
    "# Stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "\n",
    "# Two useful regex\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "hashtag_pattern = re.compile(r\"^#[0-9a-zA-Z]+\")\n",
    "\n",
    "# It's handy to have a full set of emojis\n",
    "all_language_emojis = set()\n",
    "\n",
    "for country in emoji.EMOJI_DATA : \n",
    "    for em in emoji.EMOJI_DATA[country] : \n",
    "        all_language_emojis.add(em)\n",
    "\n",
    "# and now our functions\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity, and num_tokens most common\n",
    "        tokens. Return a list of \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    \n",
    "    lexical_diversity = ld.ttr(tokens)\n",
    "    \n",
    "    num_characters = 0\n",
    "    for word in tokens:\n",
    "        num_characters+= len(word)\n",
    "        \n",
    "    common_tokens =  Counter(tokens)\n",
    "    top_five_token = common_tokens .most_common(5)\n",
    "    \n",
    "    \n",
    "    if verbose == True:        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        \n",
    "        print(f\"The five most common tokens are : {top_five_token}\")\n",
    "       \n",
    "          \n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters,top_five_token])\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "def contains_emoji(s):\n",
    "    \n",
    "    s = str(s)\n",
    "    emojis = [ch for ch in s if emoji.is_emoji(ch)]\n",
    "\n",
    "    return(len(emojis) > 0)\n",
    "\n",
    "\n",
    "def remove_stop(tokens) :\n",
    "    \n",
    "    # modify this function to remove stopwords\n",
    "    return [w for w in tokens if w.lower() not in sw]\n",
    "    \n",
    "    \n",
    "\n",
    " \n",
    "def remove_punctuation(text, punct_set=tw_punct) : \n",
    "    return(\"\".join([ch for ch in text if ch not in punct_set]))\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text) : \n",
    "    \"\"\" Splitting on whitespace rather than the book's tokenize function. That \n",
    "        function will drop tokens like '#hashtag' or '2A', which we need for Twitter. \"\"\"\n",
    "    \n",
    "    # modify this function to return tokens\n",
    "    return(text.split())\n",
    "\n",
    "\n",
    "def prepare(text, pipeline) : \n",
    "    tokens = str(text)\n",
    "    \n",
    "    for transform in pipeline : \n",
    "        tokens = transform(tokens)\n",
    "        \n",
    "    return(tokens) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Use this section to ingest your data into the data structures you plan to use. Typically this will be a dictionary or a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use the below cells as an example or read in the data in a way you prefer\n",
    "\n",
    "\n",
    "data_location = \"/Users/fatemehkhosravi/Desktop/Assignment3/\"\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "artist_files = {'cher':'cher_followers_data.txt',\n",
    "                'robyn':'robynkonichiwa_followers_data.txt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(data_location + twitter_folder + artist_files['cher'],\n",
    "                           sep=\"\\t\",\n",
    "                           quoting=3)\n",
    "\n",
    "twitter_data['artist'] = \"cher\"\n",
    "\n",
    "#twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_2 = pd.read_csv(data_location + twitter_folder + artist_files['robyn'],\n",
    "                             sep=\"\\t\",\n",
    "                             quoting=3)\n",
    "twitter_data_2['artist'] = \"robyn\"\n",
    "\n",
    "twitter_data = pd.concat([\n",
    "    twitter_data,twitter_data_2])\n",
    "    \n",
    "del(twitter_data_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the lyrics here\n",
    "\n",
    "lyrics_data= {\"artist\":[], \"song\":[],\"lyrics\": []}\n",
    "path1=data_location+lyrics_folder\n",
    "\n",
    "for artist in os.listdir(path1):\n",
    "    for song in os.listdir(os.path.join (path1+artist)):\n",
    "        song_name= song.split(\"_\")[1].split(\".\")[0]\n",
    "        with open (os.path.join(path1+artist, song),'r') as f:\n",
    "            lyric=f.read()\n",
    "        \n",
    "        lyrics_data['artist'].append(artist)\n",
    "        lyrics_data['song'].append(song_name)\n",
    "        lyrics_data['lyrics'].append(lyric)\n",
    "\n",
    "lyrics_data=pd.DataFrame(lyrics_data) \n",
    "#lyrics_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Normalization\n",
    "\n",
    "In this next section, tokenize and normalize your data. We recommend the following cleaning. \n",
    "\n",
    "**Lyrics** \n",
    "\n",
    "* Remove song titles\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords (optional)\n",
    "* Remove punctuation\n",
    "* Split on whitespace\n",
    "\n",
    "Removal of stopwords is up to you. Your descriptive statistic comparison will be different if you include stopwords, though TF-IDF should still find interesting features for you. Note that we remove stopwords before removing punctuation because the stopword set includes punctuation.\n",
    "\n",
    "**Twitter Descriptions** \n",
    "\n",
    "* Casefold to lowercase\n",
    "* Remove stopwords\n",
    "* Remove punctuation other than emojis or hashtags\n",
    "* Split on whitespace\n",
    "\n",
    "Removing stopwords seems sensible for the Twitter description data. Remember to leave in emojis and hashtags, since you analyze those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the `pipeline` techniques from BTAP Ch 1 or 5\n",
    "\n",
    "my_pipeline = [str.lower, remove_punctuation, tokenize, remove_stop]\n",
    "\n",
    "lyrics_data[\"tokens\"] = lyrics_data[\"lyrics\"].apply(prepare,pipeline=my_pipeline)\n",
    "lyrics_data[\"num_tokens\"] = lyrics_data[\"tokens\"].map(len) \n",
    "\n",
    "twitter_data[\"tokens\"] = twitter_data[\"description\"].apply(prepare,pipeline=my_pipeline)\n",
    "twitter_data[\"num_tokens\"] = twitter_data[\"tokens\"].map(len) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data['has_emoji'] = twitter_data[\"description\"].apply(contains_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at some descriptions with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>description</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1466440</th>\n",
       "      <td>cher</td>\n",
       "      <td>certified trash™ feminist</td>\n",
       "      <td>[certified, trash™, feminist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097405</th>\n",
       "      <td>cher</td>\n",
       "      <td>αℓмσѕт ιѕ иєνєя єиσυggggн ! ✌️✌️</td>\n",
       "      <td>[αℓмσѕт, ιѕ, иєνєя, єиσυggggн, ✌️✌️]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711100</th>\n",
       "      <td>cher</td>\n",
       "      <td>🖤🖤✋🏻✋🏻𝕸𝖊𝖒𝖊𝖓𝖙𝖔 𝕬𝖀𝕯𝕰𝕽𝕰 𝕾𝖊𝖒𝖕𝖊𝖗! 🇮🇹🇮🇹MEGLIO MORIRE...</td>\n",
       "      <td>[🖤🖤✋🏻✋🏻𝕸𝖊𝖒𝖊𝖓𝖙𝖔, 𝕬𝖀𝕯𝕰𝕽𝕰, 𝕾𝖊𝖒𝖕𝖊𝖗, 🇮🇹🇮🇹meglio, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153891</th>\n",
       "      <td>robyn</td>\n",
       "      <td>Stars can't shine without darkness ⭐</td>\n",
       "      <td>[stars, cant, shine, without, darkness, ⭐]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728747</th>\n",
       "      <td>cher</td>\n",
       "      <td>🌊🗽save our Republic 2020</td>\n",
       "      <td>[🌊🗽save, republic, 2020]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454765</th>\n",
       "      <td>cher</td>\n",
       "      <td>Dad to Austin &amp; Hannah, Lover to an Awesome GF...</td>\n",
       "      <td>[dad, austin, hannah, lover, awesome, gf, laki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265725</th>\n",
       "      <td>cher</td>\n",
       "      <td>Olympic level under achiever / over thinker 😄....</td>\n",
       "      <td>[olympic, level, achiever, thinker, 😄, mum, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833774</th>\n",
       "      <td>cher</td>\n",
       "      <td>@jeffshakeit fan account ✨loyle to my capo✨</td>\n",
       "      <td>[jeffshakeit, fan, account, ✨loyle, capo✨]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345478</th>\n",
       "      <td>cher</td>\n",
       "      <td>Love family, country, NY, USA ❣️#Resist #VoteB...</td>\n",
       "      <td>[love, family, country, ny, usa, ❣️#resist, #v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304382</th>\n",
       "      <td>cher</td>\n",
       "      <td>Just a Atlanta Raised lady maneuvering through...</td>\n",
       "      <td>[atlanta, raised, lady, maneuvering, thing, ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        artist                                        description  \\\n",
       "1466440   cher                          certified trash™ feminist   \n",
       "2097405   cher                   αℓмσѕт ιѕ иєνєя єиσυggggн ! ✌️✌️   \n",
       "711100    cher  🖤🖤✋🏻✋🏻𝕸𝖊𝖒𝖊𝖓𝖙𝖔 𝕬𝖀𝕯𝕰𝕽𝕰 𝕾𝖊𝖒𝖕𝖊𝖗! 🇮🇹🇮🇹MEGLIO MORIRE...   \n",
       "153891   robyn               Stars can't shine without darkness ⭐   \n",
       "1728747   cher                           🌊🗽save our Republic 2020   \n",
       "454765    cher  Dad to Austin & Hannah, Lover to an Awesome GF...   \n",
       "265725    cher  Olympic level under achiever / over thinker 😄....   \n",
       "1833774   cher        @jeffshakeit fan account ✨loyle to my capo✨   \n",
       "1345478   cher  Love family, country, NY, USA ❣️#Resist #VoteB...   \n",
       "304382    cher  Just a Atlanta Raised lady maneuvering through...   \n",
       "\n",
       "                                                    tokens  \n",
       "1466440                      [certified, trash™, feminist]  \n",
       "2097405               [αℓмσѕт, ιѕ, иєνєя, єиσυggggн, ✌️✌️]  \n",
       "711100   [🖤🖤✋🏻✋🏻𝕸𝖊𝖒𝖊𝖓𝖙𝖔, 𝕬𝖀𝕯𝕰𝕽𝕰, 𝕾𝖊𝖒𝖕𝖊𝖗, 🇮🇹🇮🇹meglio, mo...  \n",
       "153891          [stars, cant, shine, without, darkness, ⭐]  \n",
       "1728747                           [🌊🗽save, republic, 2020]  \n",
       "454765   [dad, austin, hannah, lover, awesome, gf, laki...  \n",
       "265725   [olympic, level, achiever, thinker, 😄, mum, pa...  \n",
       "1833774         [jeffshakeit, fan, account, ✨loyle, capo✨]  \n",
       "1345478  [love, family, country, ny, usa, ❣️#resist, #v...  \n",
       "304382   [atlanta, raised, lady, maneuvering, thing, ca...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data[twitter_data.has_emoji].sample(10)[[\"artist\",\"description\",\"tokens\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data processed, we can now start work on the assignment questions. \n",
    "\n",
    "Q: What is one area of improvement to your tokenization that you could theoretically carry out? (No need to actually do it; let's not make perfect the enemy of good enough.)\n",
    "\n",
    "A: Detecting other languages in twitter data and exclude the stop words for those languages. applying white space between tokens and emojis can be another improvement.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate descriptive statistics on the two sets of lyrics and compare the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35916 tokens in the data.\n",
      "There are 3703 unique tokens in the data.\n",
      "There are 172634 characters in the data.\n",
      "The lexical diversity is 0.103 in the data.\n",
      "The five most common tokens are : [('love', 1004), ('im', 513), ('know', 486), ('dont', 440), ('youre', 333)]\n"
     ]
    }
   ],
   "source": [
    "#cher lyrics\n",
    "\n",
    "song_cher=lyrics_data.loc[lyrics_data['artist']=='cher']\n",
    "song_cher=list(song_cher['tokens'])\n",
    "\n",
    "# convert nested list to a flat list\n",
    "flat_list_cher = [element for innerList in song_cher for element in innerList]\n",
    "\n",
    "assert(descriptive_stats(flat_list_cher, verbose=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15227 tokens in the data.\n",
      "There are 2156 unique tokens in the data.\n",
      "There are 73787 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n",
      "The five most common tokens are : [('know', 308), ('dont', 301), ('im', 299), ('love', 275), ('got', 251)]\n"
     ]
    }
   ],
   "source": [
    "# robyn lyrics\n",
    "\n",
    "song_robyn=lyrics_data.loc[lyrics_data['artist']=='robyn']\n",
    "song_robyn=list(song_robyn['tokens'])\n",
    "\n",
    "# convert nested list to a flat list\n",
    "flat_list_robyn = [element for innerList in song_robyn for element in innerList]\n",
    "\n",
    "assert(descriptive_stats(flat_list_robyn, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: what observations do you make about these data? \n",
    "\n",
    "A: Among 5 top words in both lyrics,  3 of them are similar: “love”, “don’t”, “know” .  Lexical diversity for Cher’s lyrics is about 0.103 and for Robyn is about 0.142. Higher value means that the lyrics contain many different word types while lower value of lexical diversity refers to the repeated words or phrases. I am not familiar with Robyn’s songs, but I know Cher’s songs are usually similar to each other, simple and easy to memorize. 0.103 also proves my assumption about Cher's songs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find tokens uniquely related to a corpus\n",
    "\n",
    "Typically we would use TF-IDF to find unique tokens in documents. Unfortunately, we either have too few documents (if we view each data source as a single document) or too many (if we view each description as a separate document). In the latter case, our problem will be that descriptions tend to be short, so our matrix would be too sparse to support analysis. \n",
    "\n",
    "To avoid these problems, we will create a custom statistic to identify words that are uniquely related to each corpus. The idea is to find words that occur often in one corpus and infrequently in the other(s). Since corpora can be of different lengths, we will focus on the _concentration_ of tokens within a corpus. \"Concentration\" is simply the count of the token divided by the total corpus length. For instance, if a corpus had length 100,000 and a word appeared 1,000 times, then the concentration would be $\\frac{1000}{100000} = 0.01$. If the same token had a concentration of $0.005$ in another corpus, then the concentration ratio would be $\\frac{0.01}{0.005} = 2$. Very rare words can easily create infinite ratios, so you will also add a cutoff to your code so that a token must appear at least $n$ times for you to return it. \n",
    "\n",
    "An example of these calculations can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1P87fkyslJhqXFnfYezNYrDrXp_GS8gwSATsZymv-9ms). Please don't hesitate to ask questions if this is confusing. \n",
    "\n",
    "In this section find 10 tokens for each of your four corpora that meet the following criteria: \n",
    "\n",
    "1. The token appears at least `n` times in all corpora\n",
    "1. The tokens are in the top 10 for the highest ratio of appearances in a given corpora vs appearances in other corpora.\n",
    "\n",
    "You will choose a cutoff for yourself based on the side of the corpus you're working with. If you're working with the Robyn-Cher corpora provided, `n=5` seems to perform reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cher lyrics\n",
    "\n",
    "cher_count_corpora={'token':[], 'corpus1 count':[], 'concentration1':[]}\n",
    "cher_tokens=lyrics_data.loc[lyrics_data['artist']=='cher','tokens']\n",
    "cher_tokens=list(cher_tokens)\n",
    "cher_tokens=[element for innerList in cher_tokens for element in innerList]\n",
    "cher_counter=Counter(cher_tokens)\n",
    "\n",
    "for t in cher_counter:\n",
    "        if cher_counter[t] >=5:\n",
    "            cher_count_corpora['token'].append(t)\n",
    "            cher_count_corpora['corpus1 count'].append(cher_counter[t])\n",
    "            concentration1 = cher_counter[t]/len(cher_tokens)\n",
    "            cher_count_corpora['concentration1'].append(concentration1)\n",
    "\n",
    "\n",
    "cher_count_corpora_df=pd.DataFrame(cher_count_corpora)\n",
    "\n",
    "  \n",
    "#robyn lyrics \n",
    "\n",
    "robyn_count_corpora={'token':[], 'corpus2 count':[], 'concentration2':[]}\n",
    "robyn_tokens=lyrics_data.loc[lyrics_data['artist']=='robyn','tokens']\n",
    "robyn_tokens=list(robyn_tokens)\n",
    "robyn_tokens=[element for innerList in robyn_tokens for element in innerList]\n",
    "robyn_counter=Counter(robyn_tokens)\n",
    "\n",
    "for t in robyn_counter:\n",
    "        if robyn_counter[t] >=5:\n",
    "            robyn_count_corpora['token'].append(t)\n",
    "            robyn_count_corpora['corpus2 count'].append(robyn_counter[t])\n",
    "            concentration2 = robyn_counter[t]/len(robyn_tokens)\n",
    "            robyn_count_corpora['concentration2'].append(concentration2)\n",
    "\n",
    "\n",
    "robyn_count_corpora_df=pd.DataFrame(robyn_count_corpora)\n",
    "\n",
    "\n",
    "#cher_count_corpora_df.shape\n",
    "#robyn_count_corpora_df.shape\n",
    "\n",
    "\n",
    "\n",
    "# merge two data frames\n",
    "\n",
    "final_count_corpora_df= cher_count_corpora_df.merge(robyn_count_corpora_df, on ='token')\n",
    "\n",
    "\n",
    "#ratio\n",
    "\n",
    "final_count_corpora_df['concentration_ratio']=final_count_corpora_df['concentration1']/final_count_corpora_df['concentration2']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>corpus1 count</th>\n",
       "      <th>concentration1</th>\n",
       "      <th>corpus2 count</th>\n",
       "      <th>concentration2</th>\n",
       "      <th>concentration_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>come</td>\n",
       "      <td>270</td>\n",
       "      <td>0.007518</td>\n",
       "      <td>72</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>1.589855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stay</td>\n",
       "      <td>78</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>2.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ill</td>\n",
       "      <td>170</td>\n",
       "      <td>0.004733</td>\n",
       "      <td>73</td>\n",
       "      <td>0.004794</td>\n",
       "      <td>0.987308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>away</td>\n",
       "      <td>196</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>39</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>2.130678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>life</td>\n",
       "      <td>124</td>\n",
       "      <td>0.003453</td>\n",
       "      <td>39</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>1.347980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token  corpus1 count  concentration1  corpus2 count  concentration2  \\\n",
       "0  come            270        0.007518             72        0.004728   \n",
       "1  stay             78        0.002172             15        0.000985   \n",
       "2   ill            170        0.004733             73        0.004794   \n",
       "3  away            196        0.005457             39        0.002561   \n",
       "4  life            124        0.003453             39        0.002561   \n",
       "\n",
       "   concentration_ratio  \n",
       "0             1.589855  \n",
       "1             2.204600  \n",
       "2             0.987308  \n",
       "3             2.130678  \n",
       "4             1.347980  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_count_corpora_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>corpus1 count</th>\n",
       "      <th>concentration1</th>\n",
       "      <th>corpus2 count</th>\n",
       "      <th>concentration2</th>\n",
       "      <th>concentration_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>walk</td>\n",
       "      <td>118</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>10.005491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>find</td>\n",
       "      <td>137</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>9.680453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>enough</td>\n",
       "      <td>129</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>7.813004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>man</td>\n",
       "      <td>213</td>\n",
       "      <td>0.005931</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>7.525316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>without</td>\n",
       "      <td>88</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>7.461722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>strong</td>\n",
       "      <td>77</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>6.529007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>live</td>\n",
       "      <td>68</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>5.765876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>believe</td>\n",
       "      <td>145</td>\n",
       "      <td>0.004037</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>5.588583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>tears</td>\n",
       "      <td>57</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>4.833161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>name</td>\n",
       "      <td>52</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>4.409199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  corpus1 count  concentration1  corpus2 count  concentration2  \\\n",
       "117     walk            118        0.003285              5        0.000328   \n",
       "188     find            137        0.003814              6        0.000394   \n",
       "183   enough            129        0.003592              7        0.000460   \n",
       "66       man            213        0.005931             12        0.000788   \n",
       "114  without             88        0.002450              5        0.000328   \n",
       "218   strong             77        0.002144              5        0.000328   \n",
       "21      live             68        0.001893              5        0.000328   \n",
       "119  believe            145        0.004037             11        0.000722   \n",
       "269    tears             57        0.001587              5        0.000328   \n",
       "110     name             52        0.001448              5        0.000328   \n",
       "\n",
       "     concentration_ratio  \n",
       "117            10.005491  \n",
       "188             9.680453  \n",
       "183             7.813004  \n",
       "66              7.525316  \n",
       "114             7.461722  \n",
       "218             6.529007  \n",
       "21              5.765876  \n",
       "119             5.588583  \n",
       "269             4.833161  \n",
       "110             4.409199  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort data frame based on 'concentration ratio ' column in descending order.\n",
    "#The tokens are in the top 10 for the highest ratio of appearances in cher lyrics corpora \n",
    "#vs appearances in robyn lyrics corpora.\n",
    "\n",
    "final_count_corpora_df.sort_values('concentration_ratio', ascending =False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>corpus1 count</th>\n",
       "      <th>concentration1</th>\n",
       "      <th>corpus2 count</th>\n",
       "      <th>concentration2</th>\n",
       "      <th>concentration_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>beat</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>146</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>0.049365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>dance</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>150</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.070660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>work</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>76</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.083677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>hang</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>38</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.089255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>forgive</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.132488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>alright</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>30</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.141320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>shake</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>18</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.141320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>party</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>21</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.161509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>space</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>20</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.169585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>pretend</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.169585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  corpus1 count  concentration1  corpus2 count  concentration2  \\\n",
       "235     beat             17        0.000473            146        0.009588   \n",
       "180    dance             25        0.000696            150        0.009851   \n",
       "290     work             15        0.000418             76        0.004991   \n",
       "346     hang              8        0.000223             38        0.002496   \n",
       "365  forgive              5        0.000139             16        0.001051   \n",
       "356  alright             10        0.000278             30        0.001970   \n",
       "90     shake              6        0.000167             18        0.001182   \n",
       "361    party              8        0.000223             21        0.001379   \n",
       "250    space              8        0.000223             20        0.001313   \n",
       "344  pretend              6        0.000167             15        0.000985   \n",
       "\n",
       "     concentration_ratio  \n",
       "235             0.049365  \n",
       "180             0.070660  \n",
       "290             0.083677  \n",
       "346             0.089255  \n",
       "365             0.132488  \n",
       "356             0.141320  \n",
       "90              0.141320  \n",
       "361             0.161509  \n",
       "250             0.169585  \n",
       "344             0.169585  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort data frame based on 'concentration ratio ' column in ascending order.\n",
    "#The tokens are in the top 10 for the highest ratio of appearances in robyn corpora \n",
    "#vs appearances in cher corpora.\n",
    "\n",
    "final_count_corpora_df.sort_values('concentration_ratio', ascending =True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What are some observations about the top tokens? Do you notice any interesting items on the list? \n",
    "\n",
    "A: Cher as a woman singer has used word of \"man\" in her songs more than Robyn. On Robyn lyrics we can see some words in a same category like “beat”, “dance”, “shake” and ”party”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word clouds for all four corpora. \n",
    "\n",
    "For building wordclouds, we'll follow exactly the code of the text. The code in this section can be found [here](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch01/First_Insights.ipynb). If you haven't already, you should absolutely clone the repository that accompanies the book. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, \n",
    "                   background_color= \"black\", colormap=\"Paired\", \n",
    "                   max_font_size=150, max_words=max_words)\n",
    "    \n",
    "    # convert data frame into dict\n",
    "    if type(word_freq) == pd.Series:\n",
    "        counter = Counter(word_freq.fillna(0).to_dict())\n",
    "    else:\n",
    "        counter = word_freq\n",
    "\n",
    "    # filter stop words in frequency counter\n",
    "    if stopwords is not None:\n",
    "        counter = {token:freq for (token, freq) in counter.items() \n",
    "                              if token not in stopwords}\n",
    "    wc.generate_from_frequencies(counter)\n",
    " \n",
    "    plt.title(title) \n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    \n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "robyn_count_words=count_words(lyrics_data[lyrics_data['artist']=='robyn'])\n",
    "cher_count_words=count_words(lyrics_data[lyrics_data['artist']=='cher'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(robyn_count_words['freq'], title = \"Robyn Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransposedFont' object has no attribute 'getbbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-3efa72915b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcher_count_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'freq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Cher Words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-52973df95955>\u001b[0m in \u001b[0;36mwordcloud\u001b[0;34m(word_freq, title, max_words, stopwords)\u001b[0m\n\u001b[1;32m     17\u001b[0m         counter = {token:freq for (token, freq) in counter.items() \n\u001b[1;32m     18\u001b[0m                               if token not in stopwords}\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    506\u001b[0m                     font, orientation=orientation)\n\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# get size of resulting text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 \u001b[0mbox_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0;31m# find possible places using integral image:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[0;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RGBA\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         bbox = font.getbbox(\n\u001b[0m\u001b[1;32m    568\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstroke_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransposedFont' object has no attribute 'getbbox'"
     ]
    }
   ],
   "source": [
    "wordcloud(cher_count_words['freq'], title = \"Cher Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What observations do you have about these (relatively straightforward) wordclouds? \n",
    "\n",
    "A: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
